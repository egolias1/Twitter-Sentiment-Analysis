{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e9c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count_vectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB as multinomial_nb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "import time\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbae8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    # load json object\n",
    "    ob = json.loads(x)\n",
    "    \n",
    "    # parse through objects in json and join as a csv or dict\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca6759ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text, weak_sentiment_word_list):\n",
    "    #Instantiate word_list array that will contain a final list of the words in the input\n",
    "    #text with the punctuation removed and all characters in lowercase.\n",
    "    word_list = []\n",
    "    \n",
    "    #Parse characters of text and remove punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    #Parse word by word in text convert to lowercase and remove stopwords and weak sentiment words\n",
    "    for word in nopunc.split():\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('english'):\n",
    "            if word not in weak_sentiment_word_list:\n",
    "                word_list.append(word.lower())\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aa8ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(tweet):\n",
    "    # instantiate yelp_normalized array\n",
    "    tweet_normalized = []\n",
    "    \n",
    "    # Create datasets separated by a normalized distribution of the number of likes\n",
    "    tweet_1 = tweet[(tweet['likes'] == 1)]\n",
    "    tweet_2 = tweet[(tweet['likes'] == 2)]\n",
    "    tweet_3 = tweet[(tweet['likes'] == 3)]\n",
    "    tweet_4 = tweet[(tweet['likes'] == 4)]\n",
    "    tweet_5 = tweet[(tweet['likes'] == 5)]\n",
    "    \n",
    "    # determine the lowest count in datasets\n",
    "    limiting_factor = min([len(tweet_1), len(tweet_2), len(tweet_3), len(tweet_4), len(tweet_5)])\n",
    "        \n",
    "    # concatenate all datasets into one dataset\n",
    "    tweet_normalized.append(tweet_1.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_2.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_3.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_4.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_5.sample(limiting_factor))\n",
    "    \n",
    "    return pd.concat(tweet_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef58d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(tweet, rating):\n",
    "    # create dataset unique to a star rating and instantiate a string array\n",
    "    tweet = tweet[(tweet['likes'] == rating)]\n",
    "    string = []\n",
    "    \n",
    "    # parse tokenized text in each review\n",
    "    for text in tweet['tokenized']:\n",
    "        # parse tokens in tokenized text and append them to string array\n",
    "        for token in text:\n",
    "            string.append(token)\n",
    "    return pd.Series(string).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd099adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(string_count, length):\n",
    "    # instantiate word_list and set count to 0\n",
    "    word_list = []\n",
    "    count = 0\n",
    "    \n",
    "    # parse words in string_count \n",
    "    for word in string_count:\n",
    "        # append the word while count is less than length\n",
    "        if count < length:\n",
    "            count += 1\n",
    "            word_list.append(word[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9beb94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weak_sentiment_list(tweet):\n",
    "    # instantiate weak_sentiment_word_list array\n",
    "    weak_sentiment_word_list = []\n",
    "\n",
    "    # generate a positive and negative string based on 1 and 5 stars\n",
    "    tweet_negative_string = generate_string(tweet, 1)\n",
    "    tweet_positive_string = generate_string(tweet, 5)\n",
    "\n",
    "    # sort items in positive and negative arrays from greatest to least\n",
    "    positive_string_count = sorted(word_count(tweet_positive_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    negative_string_count = sorted(word_count(tweet_negative_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    # arbitrarily set a length based on the length of both arrays\n",
    "    length = int((len(positive_string_count) + len(negative_string_count)) * 0.001 / 2)\n",
    "    \n",
    "    # generate positive and negative word lists\n",
    "    positive_word_list = generate_list(positive_string_count, length)\n",
    "    negative_word_list = generate_list(negative_string_count, length)\n",
    "    \n",
    "    # parse words in the lists and if they match add them to the weak sentiment array\n",
    "    for word in positive_word_list:\n",
    "        if word in negative_word_list:\n",
    "            weak_sentiment_word_list.append(word)\n",
    "    return weak_sentiment_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b83b1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_review_large_csv(override):\n",
    "    # load json into memory if it does not exist or there is an override to overwrite the file\n",
    "    if not path.exists('./dataset/review_large.csv') or override:\n",
    "        for json_filename in glob('*.json'):\n",
    "            # create csv of the same name as json\n",
    "            csv_filename = '%s.csv' % json_filename[:-5]\n",
    "            print('Converting %s to %s' % (json_filename, csv_filename))\n",
    "            \n",
    "            # parse lines of json in memory and add them to a dataframe and convert dataframe into a csv\n",
    "            df = pd.DataFrame([convert(line) for line in open(json_filename)])\n",
    "            df.to_csv(csv_filename, encoding='utf-8', index=False)\n",
    "    else:\n",
    "        print('review.csv already exists and no override detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54a4db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(tweet):\n",
    "    #Instantiate a weak_sentiment_list array\n",
    "    weak_sentiment_list = []\n",
    "    \n",
    "    #print(type(tweet))\n",
    "    \n",
    "    #Retype text to string \n",
    "    tweet['text'] = tweet['text'].astype(str)\n",
    "    \n",
    "    #Create length and tokenized columns \n",
    "    tweet['length'] = tweet['text'].apply(len)\n",
    "    tweet['tokenized'] = tweet.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    #Generate a weak sentiment word list and apply it to the tokenized column\n",
    "    weak_sentiment_list = generate_weak_sentiment_list(tweet)\n",
    "    tweet['tokenized'] = tweet.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    return tweet, weak_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07ab3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class(tweet, boundary):\n",
    "    # generate class of yelp based on stars included\n",
    "    if not boundary:\n",
    "        tweet_class = tweet\n",
    "    else:\n",
    "        tweet_class = tweet[(tweet['likes'] == 1) | (tweet['likes'] == 5)]\n",
    "    \n",
    "    return tweet_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0317b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_y(tweet_class):\n",
    "    # instantiate X_list array\n",
    "    X_list = []\n",
    "    \n",
    "    # create X and y and assign appropriate columns\n",
    "    X = tweet_class['tokenized']\n",
    "    y = tweet_class['likes']\n",
    "    \n",
    "    # parse items in column and append them to X_list array\n",
    "    for item in X:\n",
    "        X = ' '.join(item)\n",
    "        X_list.append(X)\n",
    "    \n",
    "    return X_list, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "100a3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_transformer(X):\n",
    "    # vectorize words in X with an ngram of 1 and setting a feature ceiling at 450,000 \n",
    "    # the feature ceiling was set to extend the boundary case without conflicts\n",
    "    bow_transformer = count_vectorizer(ngram_range=(1, 2), max_features=450000).fit(X)\n",
    "    \n",
    "    # transform vectorize words\n",
    "    X = bow_transformer.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "891caca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    #Instantiate a counts dictionary and split the words in str\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    #Parse words in text and count the number of times that the word appears in the dictionary\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b3e479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_train(classifier, X_train, y_train, X_test, y_test):\n",
    "    #Set time and fit classifier\n",
    "    t0 = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    #Set time and use the classifier to predict the test values  \n",
    "    t1 = time.time()\n",
    "    prediction = classifier.predict(X_test)\n",
    "    \n",
    "    #Set completion time\n",
    "    t2 = time.time()\n",
    "    \n",
    "    #Determine training and prediction times\n",
    "    time_train = t1-t0\n",
    "    time_predict = t2-t1\n",
    "    \n",
    "    #Evaluate accuracy of the classifier based on the test case\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    print('Score: {0}'.format(score))\n",
    "    print('\\n')\n",
    "    \n",
    "    # evaluate the confusion matrix based on the predictions generated by the classifier\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, prediction)\n",
    "    print('Confusion Matrix: \\n {0}'.format(confusion_matrix))\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Training time: {0:.3f}s; Prediction time: {1:.3f}s'.format(time_train, time_predict))\n",
    "    print(classification_report(y_test, prediction))\n",
    "    \n",
    "    return classifier, prediction, time_train, time_predict, score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81f1fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset of tweets\n",
    "tweets = pd.read_csv('./datasets/data_elonmusk.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c3008e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset of TSLA stock prices\n",
    "stocks = pd.read_csv('./datasets/TSLA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1fc8f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(x='Time',data=tweets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cca94bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/w2hspvvs6k19y1cddxdpvt2w0000gn/T/ipykernel_998/964015429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/y1/w2hspvvs6k19y1cddxdpvt2w0000gn/T/ipykernel_998/1683180572.py\u001b[0m in \u001b[0;36mclean_dataset\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#Retype text to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Create length and tokenized columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "clean_dataset(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a480e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elliotgolias/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: Dropping invalid columns in DataFrameGroupBy.mean is deprecated. In a future version, a TypeError will be raised. Before calling .mean, select only columns which should be valid for the function.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "retweets = tweets.groupby('Retweet from').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36de307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
