{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e5d0086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "     |████████████████████████████████| 292 kB 1.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.9/site-packages (from seaborn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/site-packages (from seaborn) (1.20.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/site-packages (from seaborn) (1.7.2)\n",
      "Collecting matplotlib>=2.2\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "     |████████████████████████████████| 7.3 MB 4.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp39-cp39-macosx_10_9_x86_64.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 1.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
      "     |████████████████████████████████| 890 kB 3.2 MB/s            \n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp39-cp39-macosx_10_10_x86_64.whl (3.0 MB)\n",
      "     |████████████████████████████████| 3.0 MB 3.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib, seaborn\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Successfully installed cycler-0.11.0 fonttools-4.28.5 kiwisolver-1.3.2 matplotlib-3.5.1 pillow-8.4.0 seaborn-0.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9e9c7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/w2hspvvs6k19y1cddxdpvt2w0000gn/T/ipykernel_24860/682287108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count_vectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB as multinomial_nb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "import time\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbae8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    # load json object\n",
    "    ob = json.loads(x)\n",
    "    \n",
    "    # parse through objects in json and join as a csv or dict\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca6759ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text, weak_sentiment_word_list):\n",
    "    # instantiate word_list array\n",
    "    word_list = []\n",
    "    \n",
    "    # parse characters of text and remove punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # parse word by word in text convert to lowercase and remove stopwords and weak sentiment words\n",
    "    for word in nopunc.split():\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('english'):\n",
    "            if word not in weak_sentiment_word_list:\n",
    "                word_list.append(word.lower())\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa8ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(tweet):\n",
    "    # instantiate yelp_normalized array\n",
    "    tweet_normalized = []\n",
    "    \n",
    "    # Create datasets separated by a normalized distribution of the number of likes\n",
    "    tweet_1 = tweet[(tweet['likes'] == 1)]\n",
    "    tweet_2 = tweet[(tweet['likes'] == 2)]\n",
    "    tweet_3 = tweet[(tweet['likes'] == 3)]\n",
    "    tweet_4 = tweet[(tweet['likes'] == 4)]\n",
    "    tweet_5 = tweet[(tweet['likes'] == 5)]\n",
    "    \n",
    "    # determine the lowest count in datasets\n",
    "    limiting_factor = min([len(tweet_1), len(tweet_2), len(tweet_3), len(tweet_4), len(tweet_5)])\n",
    "        \n",
    "    # concatenate all datasets into one dataset\n",
    "    tweet_normalized.append(tweet_1.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_2.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_3.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_4.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_5.sample(limiting_factor))\n",
    "    \n",
    "    return pd.concat(tweet_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef58d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(tweet, rating):\n",
    "    # create dataset unique to a star rating and instantiate a string array\n",
    "    tweet = tweet[(tweet['likes'] == rating)]\n",
    "    string = []\n",
    "    \n",
    "    # parse tokenized text in each review\n",
    "    for text in tweet['tokenized']:\n",
    "        # parse tokens in tokenized text and append them to string array\n",
    "        for token in text:\n",
    "            string.append(token)\n",
    "    return pd.Series(string).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd099adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(string_count, length):\n",
    "    # instantiate word_list and set count to 0\n",
    "    word_list = []\n",
    "    count = 0\n",
    "    \n",
    "    # parse words in string_count \n",
    "    for word in string_count:\n",
    "        # append the word while count is less than length\n",
    "        if count < length:\n",
    "            count += 1\n",
    "            word_list.append(word[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9beb94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weak_sentiment_list(tweet):\n",
    "    # instantiate weak_sentiment_word_list array\n",
    "    weak_sentiment_word_list = []\n",
    "\n",
    "    # generate a positive and negative string based on 1 and 5 stars\n",
    "    tweet_negative_string = generate_string(tweet, 1)\n",
    "    tweet_positive_string = generate_string(tweet, 5)\n",
    "\n",
    "    # sort items in positive and necative arrays from greatest to least\n",
    "    positive_string_count = sorted(word_count(tweet_positive_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    negative_string_count = sorted(word_count(tweet_negative_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    # arbitrarily set a length based on the length of both arrays\n",
    "    length = int((len(positive_string_count) + len(negative_string_count)) * 0.001 / 2)\n",
    "    \n",
    "    # generate positive and negative word lists\n",
    "    positive_word_list = generate_list(positive_string_count, length)\n",
    "    negative_word_list = generate_list(negative_string_count, length)\n",
    "    \n",
    "    # parse words in the lists and if they match add them to the weak sentiment array\n",
    "    for word in positive_word_list:\n",
    "        if word in negative_word_list:\n",
    "            weak_sentiment_word_list.append(word)\n",
    "    return weak_sentiment_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b83b1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_review_large_csv(override):\n",
    "    # load json into memory if it does not exist or there is an override to overwrite the file\n",
    "    if not path.exists('./dataset/review_large.csv') or override:\n",
    "        for json_filename in glob('*.json'):\n",
    "            # create csv of the same name as json\n",
    "            csv_filename = '%s.csv' % json_filename[:-5]\n",
    "            print('Converting %s to %s' % (json_filename, csv_filename))\n",
    "            \n",
    "            # parse lines of json in memory and add them to a dataframe and convert dataframe into a csv\n",
    "            df = pd.DataFrame([convert(line) for line in open(json_filename)])\n",
    "            df.to_csv(csv_filename, encoding='utf-8', index=False)\n",
    "    else:\n",
    "        print('review.csv already exists and no override detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a4db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(tweet):\n",
    "    # instantiate a weak_sentiment_list array\n",
    "    weak_sentiment_list = []\n",
    "    \n",
    "    # retype text to string \n",
    "    tweet['text'] = tweet['text'].astype(str)\n",
    "    \n",
    "    # create length and tokenized columns \n",
    "    tweet['length'] = tweet['text'].apply(len)\n",
    "    tweet['tokenized'] = tweet.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    # generate a weak sentiment word list and apply it to the tokenized column\n",
    "    weak_sentiment_list = generate_weak_sentiment_list(tweet)\n",
    "    tweet['tokenized'] = tweet.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    return tweet, weak_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07ab3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class(tweet, boundary):\n",
    "    # generate class of yelp based on stars included\n",
    "    if not boundary:\n",
    "        tweet_class = tweet\n",
    "    else:\n",
    "        tweet_class = tweet[(tweet['likes'] == 1) | (tweet['likes'] == 5)]\n",
    "    \n",
    "    return tweet_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0317b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_y(tweet_class):\n",
    "    # instantiate X_list array\n",
    "    X_list = []\n",
    "    \n",
    "    # create X and y and assign appropriate columns\n",
    "    X = tweet_class['tokenized']\n",
    "    y = tweet_class['likes']\n",
    "    \n",
    "    # parse items in column and append them to X_list array\n",
    "    for item in X:\n",
    "        X = ' '.join(item)\n",
    "        X_list.append(X)\n",
    "    \n",
    "    return X_list, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "100a3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_transformer(X):\n",
    "    # vectorize words in X with an ngram of 1 and setting a feature ceiling at 450,000 \n",
    "    # the feature ceiling was set to extend the boundary case without conflicts\n",
    "    bow_transformer = count_vectorizer(ngram_range=(1, 2), max_features=450000).fit(X)\n",
    "    \n",
    "    # transform vectorize words\n",
    "    X = bow_transformer.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "891caca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    # instantiate a counts dictionary and split the words in str\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    # parse words in text and if the word is in the dictionary increment it's value\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b3e479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_train(classifier, X_train, y_train, X_test, y_test):\n",
    "    # set time and fit classifier\n",
    "    t0 = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # set time and use the classifier to predict the test values  \n",
    "    t1 = time.time()\n",
    "    prediction = classifier.predict(X_test)\n",
    "    \n",
    "    # set completion time\n",
    "    t2 = time.time()\n",
    "    \n",
    "    # determine training and prediction times\n",
    "    time_train = t1-t0\n",
    "    time_predict = t2-t1\n",
    "    \n",
    "    # evaluate accuracy of the classifier based on the test case\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    print('Score: {0}'.format(score))\n",
    "    print('\\n')\n",
    "    \n",
    "    # evaluate the confusion matrix based on the predictions generated by the classifier\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, prediction)\n",
    "    print('Confusion Matrix: \\n {0}'.format(confusion_matrix))\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Training time: {0:.3f}s; Prediction time: {1:.3f}s'.format(time_train, time_predict))\n",
    "    print(classification_report(y_test, prediction))\n",
    "    \n",
    "    return classifier, prediction, time_train, time_predict, score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81f1fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset of tweets\n",
    "tweets = pd.read_csv('./datasets/data_elonmusk.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c3008e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset of TSLA stock prices\n",
    "stocks = pd.read_csv('./datasets/TSLA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fc8f6e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/w2hspvvs6k19y1cddxdpvt2w0000gn/T/ipykernel_24860/48912854.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.countplot(x='user',data=tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca94bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
