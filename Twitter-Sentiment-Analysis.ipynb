{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9e9c7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/w2hspvvs6k19y1cddxdpvt2w0000gn/T/ipykernel_24860/682287108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count_vectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB as multinomial_nb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "import time\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbae8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    # load json object\n",
    "    ob = json.loads(x)\n",
    "    \n",
    "    # parse through objects in json and join as a csv or dict\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4daa79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text, weak_sentiment_word_list):\n",
    "    # instantiate word_list array\n",
    "    word_list = []\n",
    "    \n",
    "    # parse characters of text and remove punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # parse word by word in text convert to lowercase and remove stopwords and weak sentiment words\n",
    "    for word in nopunc.split():\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('english'):\n",
    "            if word not in weak_sentiment_word_list:\n",
    "                word_list.append(word.lower())\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dde8a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(tweet):\n",
    "    # instantiate yelp_normalized array\n",
    "    tweet_normalized = []\n",
    "    \n",
    "    # Create datasets separated by a normalized distribution of the number of likes\n",
    "    tweet_1 = tweet[(tweet['likes'] == 1)]\n",
    "    tweet_2 = tweet[(tweet['likes'] == 2)]\n",
    "    tweet_3 = tweet[(tweet['likes'] == 3)]\n",
    "    tweet_4 = tweet[(tweet['likes'] == 4)]\n",
    "    tweet_5 = tweet[(tweet['likes'] == 5)]\n",
    "    \n",
    "    # determine the lowest count in datasets\n",
    "    limiting_factor = min([len(tweet_1), len(tweet_2), len(tweet_3), len(tweet_4), len(tweet_5)])\n",
    "        \n",
    "    # concatenate all datasets into one dataset\n",
    "    tweet_normalized.append(tweet_1.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_2.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_3.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_4.sample(limiting_factor))\n",
    "    tweet_normalized.append(tweet_5.sample(limiting_factor))\n",
    "    \n",
    "    return pd.concat(tweet_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6463dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(tweet, rating):\n",
    "    # create dataset unique to a star rating and instantiate a string array\n",
    "    tweet = tweet[(tweet['stars'] == rating)]\n",
    "    string = []\n",
    "    \n",
    "    # parse tokenized text in each review\n",
    "    for text in tweet['tokenized']:\n",
    "        # parse tokens in tokenized text and append them to string array\n",
    "        for token in text:\n",
    "            string.append(token)\n",
    "    return pd.Series(string).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "504712c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(string_count, length):\n",
    "    # instantiate word_list and set count to 0\n",
    "    word_list = []\n",
    "    count = 0\n",
    "    \n",
    "    # parse words in string_count \n",
    "    for word in string_count:\n",
    "        # append the word while count is less than length\n",
    "        if count < length:\n",
    "            count += 1\n",
    "            word_list.append(word[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4659ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weak_sentiment_list(tweet):\n",
    "    # instantiate weak_sentiment_word_list array\n",
    "    weak_sentiment_word_list = []\n",
    "\n",
    "    # generate a positive and negative string based on 1 and 5 stars\n",
    "    tweet_negative_string = generate_string(tweet, 1)\n",
    "    tweet_positive_string = generate_string(tweet, 5)\n",
    "\n",
    "    # sort items in positive and necative arrays from greatest to least\n",
    "    positive_string_count = sorted(word_count(tweet_positive_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    negative_string_count = sorted(word_count(tweet_negative_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    # arbitrarily set a length based on the length of both arrays\n",
    "    length = int((len(positive_string_count) + len(negative_string_count)) * 0.001 / 2)\n",
    "    \n",
    "    # generate positive and negative word lists\n",
    "    positive_word_list = generate_list(positive_string_count, length)\n",
    "    negative_word_list = generate_list(negative_string_count, length)\n",
    "    \n",
    "    # parse words in the lists and if they match add them to the weak sentiment array\n",
    "    for word in positive_word_list:\n",
    "        if word in negative_word_list:\n",
    "            weak_sentiment_word_list.append(word)\n",
    "    return weak_sentiment_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53c9f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_review_large_csv(override):\n",
    "    # load json into memory if it does not exist or there is an override to overwrite the file\n",
    "    if not path.exists('./dataset/review_large.csv') or override:\n",
    "        for json_filename in glob('*.json'):\n",
    "            # create csv of the same name as json\n",
    "            csv_filename = '%s.csv' % json_filename[:-5]\n",
    "            print('Converting %s to %s' % (json_filename, csv_filename))\n",
    "            \n",
    "            # parse lines of json in memory and add them to a dataframe and convert dataframe into a csv\n",
    "            df = pd.DataFrame([convert(line) for line in open(json_filename)])\n",
    "            df.to_csv(csv_filename, encoding='utf-8', index=False)\n",
    "    else:\n",
    "        print('review.csv already exists and no override detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e093f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(tweet):\n",
    "    # instantiate a weak_sentiment_list array\n",
    "    weak_sentiment_list = []\n",
    "    \n",
    "    # retype text to string \n",
    "    tweet['text'] = tweet['text'].astype(str)\n",
    "    \n",
    "    # create length and tokenized columns \n",
    "    tweet['length'] = tweet['text'].apply(len)\n",
    "    tweet['tokenized'] = tweet.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    # generate a weak sentiment word list and apply it to the tokenized column\n",
    "    weak_sentiment_list = generate_weak_sentiment_list(tweet)\n",
    "    tweet['tokenized'] = tweet.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    return tweet, weak_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a33d3c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 166584: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/w2hspvvs6k19y1cddxdpvt2w0000gn/T/ipykernel_24860/2663810656.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading the dataset of tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/data_elonmusk.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa0 in position 166584: invalid start byte"
     ]
    }
   ],
   "source": [
    "#Loading the dataset of tweets\n",
    "tweets = pd.read_csv('./datasets/data_elonmusk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d78e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
